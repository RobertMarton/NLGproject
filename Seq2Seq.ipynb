{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "uuid": "ec629cd9-aa3e-46de-818d-4433468bd445"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 40]) torch.Size([4, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "rnn = nn.GRU(10, 20, 2, bidirectional=True)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input)#[5,3,20],\n",
    "print(output.shape, hn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "uuid": "a71ea253-6ab0-4196-b6d8-b0030416854f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4584, -1.0887,  0.5371],\n",
      "        [ 1.5564, -0.6532, -0.0568]]) tensor([[-0.3872, -0.6667, -0.0190],\n",
      "        [ 0.4556, -0.0068,  0.0479]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0.4584, -1.0887,  0.5371, -0.3872, -0.6667, -0.0190],\n",
      "        [ 1.5564, -0.6532, -0.0568,  0.4556, -0.0068,  0.0479]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "y = torch.randn(2,3)\n",
    "print(x,y)\n",
    "output = torch.cat((x, y), dim=1)\n",
    "print(output.shape)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "uuid": "b851f4c6-67e7-49b8-b0ee-d224f7ccff5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3, 12]) torch.Size([3, 7])\n",
      "output: (tensor([[[ 0.0098, -0.1760, -0.1943, -0.0627, -0.0699, -0.0149,  0.4709,\n",
      "           0.0672, -0.2185,  0.1498, -0.2681,  0.3118],\n",
      "         [-0.0568,  0.2075, -0.3382,  0.3591, -0.3153, -0.4180,  0.1708,\n",
      "          -0.0518, -0.4208, -0.0412, -0.1650,  0.3432],\n",
      "         [ 0.6716,  0.0550, -0.1675,  0.2383, -0.0636, -0.2470,  0.4785,\n",
      "          -0.2991, -0.6967, -0.7147,  0.0309,  0.7958]],\n",
      "\n",
      "        [[-0.0011,  0.0274, -0.5875, -0.1444, -0.2209, -0.3688,  0.3964,\n",
      "           0.2409, -0.0312, -0.0054, -0.0987,  0.0791],\n",
      "         [ 0.2612, -0.2060,  0.1066,  0.5450, -0.1170, -0.3733, -0.1812,\n",
      "          -0.2030, -0.7021, -0.0070, -0.2136,  0.4421],\n",
      "         [ 0.8378,  0.1861, -0.3021,  0.1983, -0.3633, -0.3812,  0.2773,\n",
      "          -0.0250, -0.6029, -0.4994, -0.0520,  0.5525]]],\n",
      "       grad_fn=<CatBackward>), tensor([[ 0.3177,  0.1279, -0.2395, -0.5031,  0.1381, -0.3423, -0.1432],\n",
      "        [ 0.3131,  0.2993, -0.3893, -0.3716, -0.0898, -0.0154, -0.0125],\n",
      "        [ 0.6180,  0.3578, -0.4717, -0.4024,  0.0292,  0.1222,  0.0451]],\n",
      "       grad_fn=<TanhBackward>))\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src): \n",
    "        '''\n",
    "        src = [src_len, batch_size]\n",
    "        '''\n",
    "        src = src.transpose(0, 1) # src = [batch_size, src_len]\n",
    "#         print(\"src:\", src.shape)\n",
    "        output = self.embedding(src)\n",
    "#         print(output.shape)\n",
    "        embedded = self.dropout(self.embedding(src)).transpose(0, 1) # embedded = [src_len, batch_size, emb_dim]\n",
    "#         print(embedded.shape)\n",
    "        # enc_output = [src_len, batch_size, hid_dim * num_directions]\n",
    "        # enc_hidden = [n_layers * num_directions, batch_size, hid_dim]\n",
    "        enc_output, enc_hidden = self.rnn(embedded) # if h_0 is not give, it will be set 0 acquiescently\n",
    "#         print(enc_output.shape, enc_hidden.shape)#[2,3,12],[1*2,3,6]\n",
    "        # enc_hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        # enc_output are always from the last layer\n",
    "        \n",
    "        # enc_hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        # enc_hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        # initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        # encoder RNNs fed through a linear layer\n",
    "        # s = [batch_size, dec_hid_dim]\n",
    "        output = torch.cat((enc_hidden[-2,:,:], enc_hidden[-1,:,:]), dim = 1)\n",
    "#         print(\"output:\", output.shape)#[3,12]\n",
    "        s = torch.tanh(self.fc(output))#[3,7]\n",
    "#         print(s.shape)\n",
    "        print(enc_output.shape, s.shape)\n",
    "        return enc_output, s\n",
    "encoder = Encoder(7,5,6,7,0.0)\n",
    "# x = torch.randn(2,3)\n",
    "x = torch.tensor([[1,2,3],\n",
    "    [4,5,6]])\n",
    "print(x.shape)\n",
    "output = encoder(x)\n",
    "print(\"output:\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "uuid": "dd2ca495-dfac-473e-951b-2e782b71575e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3681],\n",
      "         [-0.0536]],\n",
      "\n",
      "        [[ 0.1666],\n",
      "         [ 0.2726]],\n",
      "\n",
      "        [[-1.5615],\n",
      "         [-0.2276]]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,2,1)\n",
    "print(x)\n",
    "output = x.squeeze(2)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "3c7a7102-aac3-419d-81d5-050cf3d4f660"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
